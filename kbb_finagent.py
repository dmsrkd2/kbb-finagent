import pandas as pd
import yfinance as yf
import numpy as np
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import json
from typing import Dict, List, Tuple, Optional
import re
import warnings
import hashlib
import os
from urllib.parse import urljoin, urlparse
import time

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.document_loaders import WebBaseLoader
from langchain.schema import Document
from langchain.chains import RetrievalQA
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain

# Chroma
import chromadb
from chromadb.config import Settings

warnings.filterwarnings('ignore')

class HyperCLOVAXLLM(LLM):
    """HyperCLOVA X APIë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ LLM í´ë˜ìŠ¤"""
    
    api_key: str
    api_url: str = "https://clovastudio.stream.ntruss.com/testapp/v1/chat-completions/HCX-003"
    
    def __init__(self, api_key: str, **kwargs):
        super().__init__(**kwargs)
        self.api_key = api_key
    
    @property
    def _llm_type(self) -> str:
        return "hyperclova_x"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
    ) -> str:
        """HyperCLOVA X API í˜¸ì¶œ"""
        try:
            headers = {
                'X-NCP-CLOVASTUDIO-API-KEY': self.api_key,
                'X-NCP-APIGW-API-KEY': self.api_key,
                'Content-Type': 'application/json'
            }
            
            payload = {
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "topP": 0.8,
                "topK": 0,
                "maxTokens": 2048,
                "temperature": 0.3,
                "repeatPenalty": 1.2,
                "stopBefore": stop or [],
                "includeAiFilters": True
            }
            
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['result']['message']['content']
            else:
                print(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return "API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"HyperCLOVA API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return "API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

class NewsVectorStore:
    """Chroma ë²¡í„° DBë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ì €ì¥ì†Œ"""
    
    def __init__(self, persist_directory: str = "./chroma_db", collection_name: str = "news_collection"):
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        
        # Chroma í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.chroma_client = chromadb.PersistentClient(path=persist_directory)
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        try:
            self.collection = self.chroma_client.get_collection(name=collection_name)
        except:
            self.collection = self.chroma_client.create_collection(
                name=collection_name,
                metadata={"hnsw:space": "cosine"}
            )
        
        # LangChain Chroma ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.vectorstore = Chroma(
            client=self.chroma_client,
            collection_name=collection_name,
            embedding_function=self.embeddings,
            persist_directory=persist_directory
        )
    
    def generate_content_hash(self, content: str) -> str:
        """ì½˜í…ì¸  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ë°©ì§€ìš©)"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def is_content_duplicate(self, content_hash: str) -> bool:
        """ì½˜í…ì¸  ì¤‘ë³µ ì—¬ë¶€ í™•ì¸"""
        try:
            results = self.collection.get(
                where={"content_hash": content_hash},
                limit=1
            )
            return len(results['ids']) > 0
        except:
            return False
    
    def search_existing_news(self, symbol: str, date: str, query: str, k: int = 5) -> List[Dict]:
        """ê¸°ì¡´ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
            docs = self.vectorstore.similarity_search_with_score(
                query=f"{symbol} {date} {query}",
                k=k,
                filter={
                    "symbol": symbol,
                    "date": date
                }
            )
            
            results = []
            for doc, score in docs:
                results.append({
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'similarity_score': score
                })
            
            return results
            
        except Exception as e:
            print(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def store_news_chunks(self, news_data: Dict, chunks: List[str]) -> bool:
        """ë‰´ìŠ¤ ì²­í¬ë¥¼ ë²¡í„° DBì— ì €ì¥"""
        try:
            documents = []
            
            for i, chunk in enumerate(chunks):
                content_hash = self.generate_content_hash(chunk)
                
                # ì¤‘ë³µ ì²´í¬
                if self.is_content_duplicate(content_hash):
                    print(f"ì¤‘ë³µ ì½˜í…ì¸  ìŠ¤í‚µ: {content_hash[:8]}...")
                    continue
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = {
                    'symbol': news_data['symbol'],
                    'date': news_data['date'],
                    'title': news_data['title'],
                    'url': news_data['url'],
                    'source': news_data['source'],
                    'chunk_index': i,
                    'content_hash': content_hash,
                    'keywords': news_data.get('keywords', []),
                    'created_at': datetime.now().isoformat()
                }
                
                # Document ê°ì²´ ìƒì„±
                doc = Document(
                    page_content=chunk,
                    metadata=metadata
                )
                
                documents.append(doc)
            
            # ë²¡í„° DBì— ì €ì¥
            if documents:
                self.vectorstore.add_documents(documents)
                print(f"âœ… {len(documents)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
                return True
            else:
                print("ì €ì¥í•  ìƒˆë¡œìš´ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            print(f"ë‰´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

class GoogleNewsSearcher:
    """êµ¬ê¸€ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, api_key: str, search_engine_id: str):
        self.api_key = api_key
        self.search_engine_id = search_engine_id
        self.base_url = "https://www.googleapis.com/customsearch/v1"
    
    def search_news(self, symbol: str, date: str, num_results: int = 10) -> List[Dict]:
        """êµ¬ê¸€ ê²€ìƒ‰ APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            date_obj = datetime.strptime(date, '%Y-%m-%d')
            date_str = date_obj.strftime('%Yë…„ %mì›” %dì¼')
            
            # í•œêµ­ ì¢…ëª©ì¸ì§€ í™•ì¸
            if symbol.endswith('.KS') or symbol.endswith('.KQ'):
                symbol_code = symbol.split('.')[0]
                query = f"{symbol_code} ì£¼ì‹ ë‰´ìŠ¤ {date_str}"
            else:
                query = f"{symbol} stock news {date}"
            
            params = {
                'key': self.api_key,
                'cx': self.search_engine_id,
                'q': query,
                'num': num_results,
                'dateRestrict': 'd7',  # 7ì¼ ì´ë‚´
                'sort': 'date'
            }
            
            response = requests.get(self.base_url, params=params)
            
            if response.status_code == 200:
                results = response.json()
                news_list = []
                
                for item in results.get('items', []):
                    news_list.append({
                        'title': item.get('title', ''),
                        'url': item.get('link', ''),
                        'snippet': item.get('snippet', ''),
                        'source': urlparse(item.get('link', '')).netloc,
                        'date': date,
                        'symbol': symbol
                    })
                
                return news_list
                
            else:
                print(f"êµ¬ê¸€ ê²€ìƒ‰ API ì˜¤ë¥˜: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

class NewsContentCrawler:
    """ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def crawl_news_content(self, url: str) -> Optional[str]:
        """ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ë³¸ë¬¸ íƒœê·¸ë“¤
            content_selectors = [
                'div.news_content',
                'div.article_body',
                'div.article-content',
                'div.entry-content',
                'div.post-content',
                'article',
                'div.content',
                'div.text'
            ]
            
            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    for elem in elements:
                        content += elem.get_text(strip=True) + " "
                    break
            
            # ê¸°ë³¸ ë³¸ë¬¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš°
            if not content:
                # p íƒœê·¸ë“¤ë¡œ ë³¸ë¬¸ êµ¬ì„±
                paragraphs = soup.find_all('p')
                content = " ".join([p.get_text(strip=True) for p in paragraphs])
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            content = re.sub(r'\s+', ' ', content)
            content = content.strip()
            
            return content if len(content) > 100 else None
            
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")
            return None

class NewsContentSummarizer:
    """ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì•½ê¸°"""
    
    def __init__(self, llm):
        self.llm = llm
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
    
    def extract_keywords(self, content: str, symbol: str) -> List[str]:
        """ë‰´ìŠ¤ ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì£¼ê°€ ê´€ë ¨ í‚¤ì›Œë“œ
        stock_keywords = ['ì£¼ê°€', 'ì£¼ì‹', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ìƒìŠ¹', 'í•˜ë½', 'ê¸‰ë“±', 'ê¸‰ë½', 
                         'í˜¸ì¬', 'ì•…ì¬', 'ì‹¤ì ', 'ìˆ˜ìµ', 'ì†ì‹¤', 'íˆ¬ì', 'ê±°ë˜ëŸ‰']
        
        # ê¸°ì—… ê´€ë ¨ í‚¤ì›Œë“œ
        company_keywords = ['ê¸°ì—…', 'íšŒì‚¬', 'ì‚¬ì—…', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ', 
                           'ì„±ì¥', 'í™•ì¥', 'ê³„ì•½', 'ì œí’ˆ', 'ì„œë¹„ìŠ¤']
        
        all_keywords = stock_keywords + company_keywords
        found_keywords = []
        
        for keyword in all_keywords:
            if keyword in content:
                found_keywords.append(keyword)
        
        # ì¢…ëª© ì½”ë“œë‚˜ ì´ë¦„ ì¶”ê°€
        if symbol:
            found_keywords.append(symbol)
        
        return found_keywords
    
    def summarize_chunks(self, chunks: List[str]) -> List[str]:
        """ì²­í¬ë³„ ìš”ì•½"""
        summarized_chunks = []
        
        for chunk in chunks:
            if len(chunk) > 500:  # ê¸¸ì´ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ ìš”ì•½
                summary_prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ë‚´ìš©ì„ ì£¼ìš” í¬ì¸íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{chunk}

ìš”ì•½:
"""
                try:
                    summary = self.llm._call(summary_prompt)
                    summarized_chunks.append(summary)
                except:
                    summarized_chunks.append(chunk)  # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
            else:
                summarized_chunks.append(chunk)
        
        return summarized_chunks

class StockDataAnalyzer:
    """ì£¼ì‹ ë°ì´í„° ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.cache = {}
    
    def get_stock_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """yfinanceë¥¼ í†µí•´ ì£¼ì‹ ë°ì´í„° ì¡°íšŒ"""
        try:
            cache_key = f"{symbol}_{start_date}_{end_date}"
            if cache_key in self.cache:
                return self.cache[cache_key]
            
            if not ('.' in symbol):
                if symbol.isdigit():
                    symbol += '.KS'
                    
            stock = yf.Ticker(symbol)
            data = stock.history(start=start_date, end=end_date)
            
            self.cache[cache_key] = data
            return data
            
        except Exception as e:
            print(f"ì£¼ì‹ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜ ({symbol}): {e}")
            return pd.DataFrame()
    
    def calculate_financial_metrics(self, symbol: str, date: str) -> Dict:
        """PER, PBR, ROE ê³„ì‚°"""
        try:
            stock = yf.Ticker(symbol)
            info = stock.info
            
            per = info.get('trailingPE', 0)
            pbr = info.get('priceToBook', 0)
            roe = info.get('returnOnEquity', 0)
            
            return {
                'PER': per if per else 0,
                'PBR': pbr if pbr else 0,
                'ROE': roe if roe else 0
            }
            
        except Exception as e:
            print(f"ì¬ë¬´ ì§€í‘œ ê³„ì‚° ì˜¤ë¥˜ ({symbol}): {e}")
            return {'PER': 0, 'PBR': 0, 'ROE': 0}
    
    def analyze_price_position(self, symbol: str, trade_date: str, period: int = 252) -> Dict:
        """ë§¤ë§¤ ì‹œì ì˜ ì£¼ê°€ ìœ„ì¹˜ ë¶„ì„"""
        try:
            end_date = datetime.strptime(trade_date, '%Y-%m-%d')
            start_date = end_date - timedelta(days=period)
            
            data = self.get_stock_data(symbol, start_date.strftime('%Y-%m-%d'), trade_date)
            
            if data.empty:
                return {'position': 'unknown', 'percentile': 0}
            
            trade_price = data.loc[trade_date, 'Close'] if trade_date in data.index else data['Close'].iloc[-1]
            
            high_price = data['High'].max()
            low_price = data['Low'].min()
            
            percentile = (trade_price - low_price) / (high_price - low_price) * 100
            
            if percentile >= 80:
                position = 'high'
            elif percentile <= 20:
                position = 'low'
            else:
                position = 'middle'
                
            return {
                'position': position,
                'percentile': percentile,
                'trade_price': trade_price,
                'high_price': high_price,
                'low_price': low_price
            }
            
        except Exception as e:
            print(f"ê°€ê²© ìœ„ì¹˜ ë¶„ì„ ì˜¤ë¥˜ ({symbol}): {e}")
            return {'position': 'unknown', 'percentile': 0}

class InvestmentAnalysisAgent:
    """ë­ì²´ì¸ ê¸°ë°˜ íˆ¬ì ë§¤ë§¤ë‚´ì—­ ë¶„ì„ AI Agent"""
    
    def __init__(self, hyperclova_api_key: str, google_api_key: str, google_search_engine_id: str):
        self.hyperclova_llm = HyperCLOVAXLLM(api_key=hyperclova_api_key)
        self.news_vectorstore = NewsVectorStore()
        self.google_searcher = GoogleNewsSearcher(google_api_key, google_search_engine_id)
        self.news_crawler = NewsContentCrawler()
        self.news_summarizer = NewsContentSummarizer(self.hyperclova_llm)
        self.stock_analyzer = StockDataAnalyzer()
        
        # ë¶„ì„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.analysis_prompt = PromptTemplate(
            input_variables=["trade_info", "financial_metrics", "price_analysis", "relevant_news"],
            template="""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë§¤ë§¤ë‚´ì—­ì„ í† ëŒ€ë¡œ íˆ¬ì ìŠµê´€ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ë§¤ë§¤ë‚´ì—­ê³¼ í•´ë‹¹ ì‹œì ì˜ PER,PBR,ROE, ì£¼ê°€ ë°ì´í„°, ë‰´ìŠ¤ë¥¼ í† ëŒ€ë¡œ ì‚¬ìš©ìê°€ í•´ë‹¹ ì¢…ëª©ì„ ë§¤ìˆ˜, ë§¤ë„í•œ ê·¼ê±°ë¥¼ ì¶”ë¡ í•´ì„œ íˆ¬ììì˜ ë§¤ë§¤ ìŠµê´€ì„ ë¶„ì„í•˜ê³  í•´ë‹¹ ë§¤ë§¤ ë°©ì‹ì˜ ì¥ì , ì•½ì , ë¦¬ìŠ¤í¬, ë³´ì™„ë°©ë²• ë“±ì„ ë¶„ì„í•´ì„œ ë¦¬í¬íŠ¸ í˜•íƒœë¡œ ì œì¶œí•´ì¤˜.

## ë§¤ë§¤ ì •ë³´
{trade_info}

## ì¬ë¬´ ì§€í‘œ
{financial_metrics}

## ì£¼ê°€ ë¶„ì„
{price_analysis}

## ê´€ë ¨ ë‰´ìŠ¤
{relevant_news}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ íˆ¬ì ìŠµê´€ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
        )
    
    def load_trading_data(self, csv_file_path: str) -> pd.DataFrame:
        """CSV íŒŒì¼ì—ì„œ ë§¤ë§¤ ë°ì´í„° ë¡œë“œ"""
        try:
            df = pd.read_csv(csv_file_path)
            df['Date'] = pd.to_datetime(df['Date'])
            return df
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
    
    def get_or_fetch_news(self, symbol: str, date: str) -> List[Dict]:
        """ë‰´ìŠ¤ ê²€ìƒ‰ ë˜ëŠ” ìˆ˜ì§‘"""
        
        # 1. ê¸°ì¡´ ë²¡í„° DBì—ì„œ ê²€ìƒ‰
        query = f"{symbol} ì£¼ì‹ ë‰´ìŠ¤"
        existing_news = self.news_vectorstore.search_existing_news(symbol, date, query)
        
        if existing_news:
            print(f"âœ… ê¸°ì¡´ ë‰´ìŠ¤ ë°œê²¬: {len(existing_news)}ê°œ")
            return existing_news
        
        # 2. êµ¬ê¸€ ê²€ìƒ‰ APIë¡œ ìƒˆ ë‰´ìŠ¤ ìˆ˜ì§‘
        print(f"ğŸ” ìƒˆ ë‰´ìŠ¤ ê²€ìƒ‰: {symbol} - {date}")
        news_results = self.google_searcher.search_news(symbol, date)
        
        if not news_results:
            print("âŒ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 3. ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ì²˜ë¦¬
        processed_news = []
        for news in news_results[:5]:  # ìƒìœ„ 5ê°œë§Œ ì²˜ë¦¬
            print(f"ğŸ“° í¬ë¡¤ë§ ì¤‘: {news['title'][:50]}...")
            
            # ë³¸ë¬¸ í¬ë¡¤ë§
            content = self.news_crawler.crawl_news_content(news['url'])
            if not content:
                continue
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.news_summarizer.extract_keywords(content, symbol)
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self.news_summarizer.text_splitter.split_text(content)
            
            # ì²­í¬ ìš”ì•½
            summarized_chunks = self.news_summarizer.summarize_chunks(chunks)
            
            # ë‰´ìŠ¤ ë°ì´í„° êµ¬ì„±
            news_data = {
                'symbol': symbol,
                'date': date,
                'title': news['title'],
                'url': news['url'],
                'source': news['source'],
                'keywords': keywords,
                'content': content
            }
            
            # ë²¡í„° DBì— ì €ì¥
            if self.news_vectorstore.store_news_chunks(news_data, summarized_chunks):
                processed_news.append({
                    'title': news['title'],
                    'content': ' '.join(summarized_chunks[:3]),  # ìƒìœ„ 3ê°œ ì²­í¬ë§Œ
                    'keywords': keywords,
                    'relevance_score': self.calculate_relevance_score(content, symbol)
                })
            
            time.sleep(1)  # í¬ë¡¤ë§ ê°„ê²© ì¡°ì ˆ
        
        return processed_news
    
    def calculate_relevance_score(self, content: str, symbol: str) -> float:
        """ë‰´ìŠ¤ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        positive_keywords = ['ìƒìŠ¹', 'ê¸‰ë“±', 'í˜¸ì¬', 'ë§¤ìˆ˜', 'íˆ¬ì', 'ì„±ì¥', 'ìˆ˜ìµ', 'ì¦ê°€']
        negative_keywords = ['í•˜ë½', 'ê¸‰ë½', 'ì•…ì¬', 'ë§¤ë„', 'ì†ì‹¤', 'ê°ì†Œ', 'ë¶€ì§„', 'ìœ„í—˜']
        
        content_lower = content.lower()
        
        positive_score = sum(1 for keyword in positive_keywords if keyword in content_lower)
        negative_score = sum(1 for keyword in negative_keywords if keyword in content_lower)
        symbol_mention = 2 if symbol.lower() in content_lower else 0
        
        return (positive_score + negative_score + symbol_mention) / 10
    
    def analyze_single_trade(self, trade_row: pd.Series) -> Dict:
        """ë‹¨ì¼ ë§¤ë§¤ ê±´ ë¶„ì„"""
        symbol = trade_row['Symbol']
        date = trade_row['Date'].strftime('%Y-%m-%d')
        trade_type = trade_row['TradeType']
        price = trade_row['Price']
        quantity = trade_row['Quantity']
        
        print(f"\nğŸ“Š ë¶„ì„ ì¤‘: {symbol} - {date} ({trade_type})")
        
        # 1. ì¬ë¬´ ì§€í‘œ ê³„ì‚°
        financial_metrics = self.stock_analyzer.calculate_financial_metrics(symbol, date)
        
        # 2. ì£¼ê°€ ìœ„ì¹˜ ë¶„ì„
        price_analysis = self.stock_analyzer.analyze_price_position(symbol, date)
        
        # 3. ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
        relevant_news = self.get_or_fetch_news(symbol, date)
        
        return {
            'symbol': symbol,
            'date': date,
            'trade_type': trade_type,
            'price': price,
            'quantity': quantity,
            'financial_metrics': financial_metrics,
            'price_analysis': price_analysis,
            'relevant_news': relevant_news
        }
    
    def generate_analysis_report(self, trade_analysis: Dict) -> str:
        """LangChainì„ ì‚¬ìš©í•œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # ì…ë ¥ ë°ì´í„° í¬ë§·íŒ…
        trade_info = f"""
- ì¢…ëª©: {trade_analysis['symbol']}
- ë‚ ì§œ: {trade_analysis['date']}
- ë§¤ë§¤ ìœ í˜•: {trade_analysis['trade_type']}
- ê°€ê²©: {trade_analysis['price']:,.0f}ì›
- ìˆ˜ëŸ‰: {trade_analysis['quantity']}ì£¼
"""
        
        metrics = trade_analysis['financial_metrics']
        financial_metrics = f"""
- PER: {metrics['PER']:.2f}
- PBR: {metrics['PBR']:.2f}
- ROE: {metrics['ROE']:.2f}%
"""
        
        price_analysis = trade_analysis['price_analysis']
        price_info = f"""
- ê°€ê²© ìœ„ì¹˜: {price_analysis['position']} (ë°±ë¶„ìœ„: {price_analysis['percentile']:.1f}%)
- ë§¤ë§¤ê°€: {price_analysis.get('trade_price', 0):,.0f}ì›
- ê¸°ê°„ ìµœê³ ê°€: {price_analysis.get('high_price', 0):,.0f}ì›
- ê¸°ê°„ ìµœì €ê°€: {price_analysis.get('low_price', 0):,.0f}ì›
"""
        
        news_info = ""
        for i, news in enumerate(trade_analysis['relevant_news'][:3], 1):
            news_info += f"{i}. {news['title']}\n"
            news_info += f"   í‚¤ì›Œë“œ: {', '.join(news.get('keywords', [])[:5])}\n"
            news_info += f"   ê´€ë ¨ë„: {news.get('relevance_score', 0):.2f}\n\n"
        
        if not trade_analysis['relevant_news']:
            news_info = "í•´ë‹¹ ë‚ ì§œì— ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì‹¤í–‰
        prompt = self.analysis_prompt.format(
            trade_info=trade_info,
            financial_metrics=financial_metrics,
            price_analysis=price_info,
            relevant_news=news_info
        )
        
        return self.hyperclova_llm._call(prompt)
    
    def analyze_trading_patterns(self, csv_file_path: str) -> str:
        """ì „ì²´ ë§¤ë§¤ íŒ¨í„´ ë¶„ì„"""
        
        # ë§¤ë§¤ ë°ì´í„° ë¡œë“œ
        trading_data = self.load_trading_data(csv_file_path)
        if trading_data.empty:
            return "ë§¤ë§¤ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"ğŸ“ˆ ì´ {len(trading_data)}ê±´ì˜ ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì‹œì‘...")
        
        # ê° ë§¤ë§¤ ê±´ë³„ ë¶„ì„
        analysis_results = []
        
        for idx, trade_row in trading_data.iterrows():
            try:
                # ë‹¨ì¼ ë§¤ë§¤ ë¶„ì„
                trade_analysis = self.analyze_single_trade(trade_row)
                
                # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
                analysis_report = self.generate_analysis_report(trade_analysis)
                
                analysis_results.append({
                    'trade_info': trade_analysis,
                    'analysis_report': analysis_report
                })
                
                print(f"âœ… ì™„ë£Œ: {idx+1}/{len(trading_data)}")
                
            except Exception as e:
                print(f"âŒ ë¶„ì„ ì˜¤ë¥˜ ({idx+1}): {e}")
                continue
        
        # ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        comprehensive_report = self.generate_comprehensive_report(analysis_results)
        
        return comprehensive_report
    
    def generate_comprehensive_report(self, analysis_results: List[Dict]) -> str:
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        if not analysis_results:
            return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        report = f"""
# ğŸ¯ íˆ¬ì ìŠµê´€ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“Š ê°œìš”
- ë¶„ì„ ê¸°ê°„: {len(analysis_results)}ê±´ì˜ ë§¤ë§¤ ë‚´ì—­
- ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“ˆ ë§¤ë§¤ íŒ¨í„´ ë¶„ì„
"""
        
        # ë§¤ë§¤ í†µê³„
        total_trades = len(analysis_results)
        buy_count = sum(1 for r in analysis_results if r['trade_info']['trade_type'] == 'Buy')
        sell_count = total_trades - buy_count
        
        # ì¢…ëª© ë¶„ì„
        symbols = [r['trade_info']['symbol'] for r in analysis_results]
        unique_symbols = list(set(symbols))
        symbol_counts = {symbol: symbols.count(symbol) for symbol in unique_symbols}
        
        report += f"""
### ë§¤ë§¤ í˜„í™©
- ì´ ë§¤ë§¤ ê±´ìˆ˜: {total_trades}ê±´
- ë§¤ìˆ˜: {buy_count}ê±´ ({buy_count/total_trades*100:.1f}%)
- ë§¤ë„: {sell_count}ê±´ ({sell_count/total_trades*100:.1f}%)

### ê±°ë˜ ì¢…ëª© í˜„í™©
- ê±°ë˜ ì¢…ëª© ìˆ˜: {len(unique_symbols)}ê°œ
- ì£¼ìš” ê±°ë˜ ì¢…ëª©:
"""
        
        # ìƒìœ„ ê±°ë˜ ì¢…ëª©
        sorted_symbols = sorted(symbol_counts.items(), key=lambda x: x[1], reverse=True)
        for symbol, count in sorted_symbols[:5]:
            report += f"  - {symbol}: {count}ê±´\n"
        
        # ê°€ê²© ìœ„ì¹˜ ë¶„ì„
        price_positions = [r['trade_info']['price_analysis']['position'] for r in analysis_results]
        high_count = price_positions.count('high')
        middle_count = price_positions.count('middle')
        low_count = price_positions.count('low')
        
        report += f"""
### ë§¤ë§¤ ì‹œì  ë¶„ì„
- ê³ ê°€ê¶Œ ë§¤ë§¤: {high_count}ê±´ ({high_count/total_trades*100:.1f}%)
- ì¤‘ê°„ê¶Œ ë§¤ë§¤: {middle_count}ê±´ ({middle_count/total_trades*100:.1f}%)
- ì €ê°€ê¶Œ ë§¤ë§¤: {low_count}ê±´ ({low_count/total_trades*100:.1f}%)
"""
        
        # ê°œë³„ ë§¤ë§¤ ë¶„ì„ ìš”ì•½
        report += "\n## ğŸ“ ê°œë³„ ë§¤ë§¤ ë¶„ì„\n\n"
        
        for i, result in enumerate(analysis_results, 1):
            trade_info = result['trade_info']
            report += f"### {i}. {trade_info['symbol']} - {trade_info['date']} ({trade_info['trade_type']})\n"
            report += f"**ê°€ê²©:** {trade_info['price']:,.0f}ì› / **ìˆ˜ëŸ‰:** {trade_info['quantity']}ì£¼\n"
            report += f"**ìœ„ì¹˜:** {trade_info['price_analysis']['position']} (ë°±ë¶„ìœ„: {trade_info['price_analysis']['percentile']:.1f}%)\n"
            report += f"**ì¬ë¬´ì§€í‘œ:** PER {trade_info['financial_metrics']['PER']:.2f}, "
            report += f"PBR {trade_info['financial_metrics']['PBR']:.2f}, "
            report += f"ROE {trade_info['financial_metrics']['ROE']:.2f}%\n"
            
            if trade_info['relevant_news']:
                report += f"**ê´€ë ¨ ë‰´ìŠ¤:** {len(trade_info['relevant_news'])}ê±´\n"
            
            report += "\n**ë¶„ì„ ê²°ê³¼:**\n"
            report += result['analysis_report']
            report += "\n" + "="*50 + "\n\n"
        
        # ì¢…í•© íˆ¬ì ìŠµê´€ ë¶„ì„
        comprehensive_analysis = self.generate_final_comprehensive_analysis(analysis_results)
        report += f"\n## ğŸ¯ ì¢…í•© íˆ¬ì ìŠµê´€ ë¶„ì„\n\n{comprehensive_analysis}"
        
        return report
    
    def generate_final_comprehensive_analysis(self, analysis_results: List[Dict]) -> str:
        """ìµœì¢… ì¢…í•© ë¶„ì„"""
        
        # ì „ì²´ ë§¤ë§¤ íŒ¨í„´ ìš”ì•½
        patterns_summary = self.summarize_trading_patterns(analysis_results)
        
        # ì¢…í•© ë¶„ì„ í”„ë¡¬í”„íŠ¸
        comprehensive_prompt = f"""
ë‹¹ì‹ ì€ íˆ¬ì ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒê³¼ ê°™ì€ ë§¤ë§¤ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ í† ëŒ€ë¡œ íˆ¬ììì˜ ì „ë°˜ì ì¸ íˆ¬ì ìŠµê´€ì„ ì¢…í•© ë¶„ì„í•´ì£¼ì„¸ìš”:

## ë§¤ë§¤ íŒ¨í„´ ìš”ì•½
{patterns_summary}

## ê°œë³„ ë§¤ë§¤ ë¶„ì„ ê²°ê³¼
{self.extract_key_insights(analysis_results)}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **íˆ¬ì ìŠ¤íƒ€ì¼ ë¶„ì„** - ê°€ì¹˜íˆ¬ì, ì„±ì¥íˆ¬ì, ê¸°ìˆ ì  ë¶„ì„ ë“± ì–´ë–¤ ìŠ¤íƒ€ì¼ì¸ì§€
2. **ë§¤ë§¤ íƒ€ì´ë° ë¶„ì„** - ê³ ì /ì €ì  ë§¤ë§¤ íŒ¨í„´, ì‹œì¥ íƒ€ì´ë° ëŠ¥ë ¥
3. **ì¢…ëª© ì„ íƒ ê¸°ì¤€** - ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ì¢…ëª©ì„ ì„ íƒí•˜ëŠ”ì§€ ì¶”ë¡ 
4. **ë¦¬ìŠ¤í¬ ê´€ë¦¬** - í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì‚°, ì†ì ˆ/ìµì ˆ íŒ¨í„´
5. **ê°•ì ê³¼ ì•½ì ** - í˜„ì¬ íˆ¬ì ë°©ì‹ì˜ ì¥ë‹¨ì 
6. **ê°œì„  ë°©ì•ˆ** - êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ

ë¶„ì„ ê²°ê³¼ë¥¼ ì²´ê³„ì ì´ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
        
        return self.hyperclova_llm._call(comprehensive_prompt)
    
    def summarize_trading_patterns(self, analysis_results: List[Dict]) -> str:
        """ë§¤ë§¤ íŒ¨í„´ ìš”ì•½"""
        
        total_trades = len(analysis_results)
        
        # ë§¤ë§¤ íƒ€ì…ë³„ ë¶„ì„
        buy_trades = [r for r in analysis_results if r['trade_info']['trade_type'] == 'Buy']
        sell_trades = [r for r in analysis_results if r['trade_info']['trade_type'] == 'Sell']
        
        # ê°€ê²© ìœ„ì¹˜ ë¶„ì„
        buy_positions = [r['trade_info']['price_analysis']['position'] for r in buy_trades]
        sell_positions = [r['trade_info']['price_analysis']['position'] for r in sell_trades]
        
        # ì¬ë¬´ì§€í‘œ ë¶„ì„
        avg_per = np.mean([r['trade_info']['financial_metrics']['PER'] for r in analysis_results if r['trade_info']['financial_metrics']['PER'] > 0])
        avg_pbr = np.mean([r['trade_info']['financial_metrics']['PBR'] for r in analysis_results if r['trade_info']['financial_metrics']['PBR'] > 0])
        avg_roe = np.mean([r['trade_info']['financial_metrics']['ROE'] for r in analysis_results if r['trade_info']['financial_metrics']['ROE'] > 0])
        
        summary = f"""
### ë§¤ë§¤ íŒ¨í„´ í†µê³„
- ì´ ë§¤ë§¤: {total_trades}ê±´
- ë§¤ìˆ˜: {len(buy_trades)}ê±´, ë§¤ë„: {len(sell_trades)}ê±´

### ë§¤ìˆ˜ íŒ¨í„´
- ê³ ê°€ê¶Œ ë§¤ìˆ˜: {buy_positions.count('high')}ê±´
- ì¤‘ê°„ê¶Œ ë§¤ìˆ˜: {buy_positions.count('middle')}ê±´  
- ì €ê°€ê¶Œ ë§¤ìˆ˜: {buy_positions.count('low')}ê±´

### ë§¤ë„ íŒ¨í„´
- ê³ ê°€ê¶Œ ë§¤ë„: {sell_positions.count('high')}ê±´
- ì¤‘ê°„ê¶Œ ë§¤ë„: {sell_positions.count('middle')}ê±´
- ì €ê°€ê¶Œ ë§¤ë„: {sell_positions.count('low')}ê±´

### í‰ê·  ì¬ë¬´ì§€í‘œ
- í‰ê·  PER: {avg_per:.2f}
- í‰ê·  PBR: {avg_pbr:.2f}
- í‰ê·  ROE: {avg_roe:.2f}%
"""
        
        return summary
    
    def extract_key_insights(self, analysis_results: List[Dict]) -> str:
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        
        insights = []
        
        for result in analysis_results:
            trade_info = result['trade_info']
            analysis = result['analysis_report']
            
            # ì£¼ìš” íŠ¹ì§• ì¶”ì¶œ
            key_features = {
                'symbol': trade_info['symbol'],
                'trade_type': trade_info['trade_type'],
                'price_position': trade_info['price_analysis']['position'],
                'per': trade_info['financial_metrics']['PER'],
                'news_count': len(trade_info['relevant_news'])
            }
            
            insights.append(key_features)
        
        # ì¸ì‚¬ì´íŠ¸ ìš”ì•½
        summary = "ì£¼ìš” ë§¤ë§¤ íŠ¹ì§•:\n"
        for i, insight in enumerate(insights, 1):
            summary += f"{i}. {insight['symbol']} {insight['trade_type']} - "
            summary += f"{insight['price_position']}ê°€ê¶Œ, PER {insight['per']:.1f}, "
            summary += f"ë‰´ìŠ¤ {insight['news_count']}ê±´\n"
        
        return summary
    
    def save_analysis_report(self, report: str, filename: str = None) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥"""
        
        if filename is None:
            filename = f"investment_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"âœ… ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì˜¤ë¥˜: {e}")
            return ""

# ì‚¬ìš© ì˜ˆì‹œ
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # API í‚¤ ì„¤ì • (ì‹¤ì œ ì‚¬ìš© ì‹œ í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ)
    HYPERCLOVA_API_KEY = "your_hyperclova_api_key"
    GOOGLE_API_KEY = "your_google_api_key"
    GOOGLE_SEARCH_ENGINE_ID = "your_google_search_engine_id"
    
    # OpenAI API í‚¤ ì„¤ì • (embeddingsìš©)
    os.environ["OPENAI_API_KEY"] = "your_openai_api_key"
    
    # AI Agent ì´ˆê¸°í™”
    agent = InvestmentAnalysisAgent(
        hyperclova_api_key=HYPERCLOVA_API_KEY,
        google_api_key=GOOGLE_API_KEY,
        google_search_engine_id=GOOGLE_SEARCH_ENGINE_ID
    )
    
    # ë§¤ë§¤ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    csv_file_path = "trading_data.csv"
    
    try:
        print("ğŸš€ íˆ¬ì ë§¤ë§¤ë‚´ì—­ ë¶„ì„ AI Agent ì‹œì‘")
        print("="*60)
        
        # ë¶„ì„ ì‹¤í–‰
        analysis_report = agent.analyze_trading_patterns(csv_file_path)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š ë¶„ì„ ì™„ë£Œ!")
        print("="*60)
        print(analysis_report)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        saved_file = agent.save_analysis_report(analysis_report)
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ë¦¬í¬íŠ¸ íŒŒì¼: {saved_file}")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
class TradingDataValidator:
    """ë§¤ë§¤ ë°ì´í„° ê²€ì¦"""
    
    @staticmethod
    def validate_csv_format(df: pd.DataFrame) -> bool:
        """CSV í˜•ì‹ ê²€ì¦"""
        required_columns = ['Date', 'Symbol', 'TradeType', 'Price', 'Quantity']
        
        for col in required_columns:
            if col not in df.columns:
                print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {col}")
                return False
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        try:
            df['Date'] = pd.to_datetime(df['Date'])
            df['Price'] = pd.to_numeric(df['Price'])
            df['Quantity'] = pd.to_numeric(df['Quantity'])
        except Exception as e:
            print(f"âŒ ë°ì´í„° íƒ€ì… ì˜¤ë¥˜: {e}")
            return False
        
        # TradeType ê²€ì¦
        valid_trade_types = ['Buy', 'Sell', 'buy', 'sell']
        if not df['TradeType'].isin(valid_trade_types).all():
            print("âŒ TradeTypeì€ 'Buy' ë˜ëŠ” 'Sell'ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return False
        
        print("âœ… CSV í˜•ì‹ ê²€ì¦ ì™„ë£Œ")
        return True
    
    @staticmethod
    def clean_symbol_format(symbol: str) -> str:
        """ì¢…ëª© ì½”ë“œ í˜•ì‹ ì •ë¦¬"""
        symbol = symbol.upper().strip()
        
        # í•œêµ­ ì¢…ëª© ì²˜ë¦¬
        if symbol.isdigit():
            if len(symbol) == 6:
                # 6ìë¦¬ ìˆ«ìì¸ ê²½ìš° KS/KQ ìë™ íŒë³„ (ì„ì‹œ)
                return f"{symbol}.KS"
            
        return symbol

class PerformanceTracker:
    """ì„±ê³¼ ì¶”ì """
    
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.processing_times = {}
    
    def start_tracking(self, task_name: str):
        """ì¶”ì  ì‹œì‘"""
        self.processing_times[task_name] = time.time()
    
    def end_tracking(self, task_name: str):
        """ì¶”ì  ì¢…ë£Œ"""
        if task_name in self.processing_times:
            elapsed = time.time() - self.processing_times[task_name]
            print(f"â±ï¸ {task_name}: {elapsed:.2f}ì´ˆ")
            return elapsed
        return 0
    
    def get_summary(self) -> str:
        """ì„±ê³¼ ìš”ì•½"""
        total_time = sum(self.processing_times.values())
        summary = f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\n"
        
        for task, duration in self.processing_times.items():
            percentage = (duration / total_time) * 100
            summary += f"- {task}: {duration:.2f}ì´ˆ ({percentage:.1f}%)\n"
        
        return summary

# ì„¤ì • íŒŒì¼ í…œí”Œë¦¿
CONFIG_TEMPLATE = """
# íˆ¬ì ë§¤ë§¤ë‚´ì—­ ë¶„ì„ AI Agent ì„¤ì • íŒŒì¼

# API í‚¤ ì„¤ì •
HYPERCLOVA_API_KEY = "your_hyperclova_api_key_here"
GOOGLE_API_KEY = "your_google_api_key_here"
GOOGLE_SEARCH_ENGINE_ID = "your_google_search_engine_id_here"
OPENAI_API_KEY = "your_openai_api_key_here"

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
CHROMA_DB_PATH = "./chroma_db"
CHROMA_COLLECTION_NAME = "investment_news"

# ë¶„ì„ ì„¤ì •
ANALYSIS_PERIOD_DAYS = 252  # 1ë…„
MAX_NEWS_PER_SYMBOL = 5
NEWS_RELEVANCE_THRESHOLD = 0.3

# í¬ë¡¤ë§ ì„¤ì •
CRAWLING_DELAY = 1  # ì´ˆ
REQUEST_TIMEOUT = 10  # ì´ˆ
MAX_RETRIES = 3
"""

def create_config_file(filename: str = "config.py"):
    """ì„¤ì • íŒŒì¼ ìƒì„±"""
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(CONFIG_TEMPLATE)
    print(f"âœ… ì„¤ì • íŒŒì¼ ìƒì„±: {filename}")

def create_sample_data(filename: str = "sample_trading_data.csv"):
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    sample_data = {
        'Date': ['2024-01-15', '2024-01-20', '2024-02-01', '2024-02-15', '2024-03-01'],
        'Symbol': ['AAPL', '005930.KS', 'GOOGL', '000660.KS', 'TSLA'],
        'TradeType': ['Buy', 'Buy', 'Sell', 'Buy', 'Sell'],
        'Price': [150.00, 75000, 2800.00, 45000, 220.00],
        'Quantity': [100, 50, 25, 80, 150]
    }
    
    df = pd.DataFrame(sample_data)
    df.to_csv(filename, index=False)
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„±: {filename}")

# ì‹¤í–‰ ë„ìš°ë¯¸
def setup_environment():
    """í™˜ê²½ ì„¤ì • ë„ìš°ë¯¸"""
    print("ğŸ”§ í™˜ê²½ ì„¤ì • ì‹œì‘...")
    
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("./chroma_db", exist_ok=True)
    os.makedirs("./reports", exist_ok=True)
    
    # ì„¤ì • íŒŒì¼ ìƒì„± (ì—†ëŠ” ê²½ìš°)
    if not os.path.exists("config.py"):
        create_config_file()
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì—†ëŠ” ê²½ìš°)
    if not os.path.exists("sample_trading_data.csv"):
        create_sample_data()
    
    print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!")
    print("ğŸ“ config.py íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    print("ğŸ“Š sample_trading_data.csv íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")

if __name__ == "__main__":
    # í™˜ê²½ ì„¤ì •
    setup_environment()
    
    # ë©”ì¸ ì‹¤í–‰
    main()